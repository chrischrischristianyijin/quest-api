"""
TF-IDF æ–‡æœ¬é¢„å¤„ç†ä¼˜åŒ–å™¨

åœ¨ Trafilatura æå–ä¹‹å‰ä½¿ç”¨ TF-IDF å‘é‡åŒ–è¿›è¡Œå†…å®¹ä¼˜åŒ–ï¼š
1. è¯†åˆ«é¡µé¢ä¸­çš„é‡è¦æ–‡æœ¬å—
2. è¿‡æ»¤ä½è´¨é‡å†…å®¹ï¼ˆå¹¿å‘Šã€å¯¼èˆªç­‰ï¼‰
3. æå–æ ¸å¿ƒå†…å®¹åŒºåŸŸ
4. ä¼˜åŒ– HTML ç»“æ„ä»¥æé«˜ Trafilatura æå–è´¨é‡
"""

import os
import re
import logging
from typing import Dict, List, Optional, Tuple, Any
from bs4 import BeautifulSoup, Tag
import numpy as np

logger = logging.getLogger(__name__)

# æ£€æŸ¥ scikit-learn å¯ç”¨æ€§
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity, linear_kernel
    SKLEARN_AVAILABLE = True
    logger.info("scikit-learn åº“åŠ è½½æˆåŠŸ")
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("scikit-learn ä¸å¯ç”¨ï¼ŒTF-IDF ä¼˜åŒ–å°†è¢«è·³è¿‡")
    TfidfVectorizer = None
    cosine_similarity = None
    linear_kernel = None

# æ£€æŸ¥ jieba ä¸­æ–‡åˆ†è¯å¯ç”¨æ€§
try:
    import jieba
    JIEBA_AVAILABLE = True
    logger.info("jieba ä¸­æ–‡åˆ†è¯åº“åŠ è½½æˆåŠŸ")
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("jieba ä¸å¯ç”¨ï¼Œä¸­æ–‡åˆ†è¯å°†è¢«è·³è¿‡")
    jieba = None


def is_tfidf_enabled() -> bool:
    """æ£€æŸ¥æ˜¯å¦å¯ç”¨ TF-IDF ä¼˜åŒ–"""
    return (
        SKLEARN_AVAILABLE and 
        os.getenv('TFIDF_OPTIMIZER_ENABLED', 'true').lower() in ('1', 'true', 'yes', 'on')
    )


def get_tfidf_config() -> Dict[str, Any]:
    """è·å– TF-IDF ä¼˜åŒ–é…ç½®"""
    return {
        'min_text_length': int(os.getenv('TFIDF_MIN_TEXT_LENGTH', '80') or '80'),  # ä¸­æ–‡å»ºè®® 80-120
        'max_features': int(os.getenv('TFIDF_MAX_FEATURES', '10000') or '10000'),  # å¢åŠ åˆ° 10k
        'min_df': int(os.getenv('TFIDF_MIN_DF', '2') or '2'),  # æ”¹ä¸ºç»å¯¹è®¡æ•°ï¼š2
        'max_df': float(os.getenv('TFIDF_MAX_DF', '0.8') or '0.8'),
        'score_floor': float(os.getenv('TFIDF_SCORE_FLOOR', '0.06') or '0.06'),  # é‡å‘½åï¼šå¾—åˆ†ä¸‹é™
        'content_ratio_threshold': float(os.getenv('TFIDF_CONTENT_RATIO', '0.2') or '0.2'),  # é™ä½åˆ° 20%
        'min_keep_k': int(os.getenv('TFIDF_MIN_KEEP_K', '80') or '80'),  # æ–°å¢ï¼šæœ€ä½ä¿ç•™å—æ•°
        'percentile_threshold': float(os.getenv('TFIDF_PERCENTILE_THRESHOLD', '0.80') or '0.80'),  # æ–°å¢ï¼š80åˆ†ä½é˜ˆå€¼
        'remove_low_quality': os.getenv('TFIDF_REMOVE_LOW_QUALITY', 'true').lower() in ('1', 'true', 'yes'),
        'enhance_content_blocks': os.getenv('TFIDF_ENHANCE_BLOCKS', 'true').lower() in ('1', 'true', 'yes'),
        'enable_chinese_segmentation': os.getenv('TFIDF_ENABLE_CHINESE_SEG', 'true').lower() in ('1', 'true', 'yes'),
        'max_link_density': float(os.getenv('TFIDF_MAX_LINK_DENSITY', '0.3') or '0.3'),  # æœ€å¤§é“¾æ¥å¯†åº¦
        'min_alphanumeric_ratio': float(os.getenv('TFIDF_MIN_ALPHANUMERIC_RATIO', '0.5') or '0.5'),  # æœ€å°å­—æ¯æ•°å­—æ¯”ä¾‹
    }


class TfidfTextOptimizer:
    """TF-IDF æ–‡æœ¬ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or get_tfidf_config()
        self.vectorizer = None
        self.content_vectors = None
        
    def _extract_text_blocks(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """æå–é¡µé¢ä¸­çš„æ–‡æœ¬å—ï¼Œæ”¯æŒåˆ†åŒºæ®µä¿åº•æœºåˆ¶"""
        text_blocks = []
        
        # å®šä¹‰å†…å®¹æ ‡ç­¾
        content_tags = ['p', 'div', 'article', 'section', 'main', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']
        
        # å®šä¹‰éœ€è¦æ’é™¤çš„æ ‡ç­¾
        exclude_tags = ['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu', 'noscript']
        
        # ç§»é™¤æ’é™¤æ ‡ç­¾
        for tag_name in exclude_tags:
            for tag in soup.find_all(tag_name):
                tag.decompose()
        
        # æŒ‰æ®µè½åˆ†ç»„ï¼ˆåŸºäºäºŒçº§æ ‡é¢˜æˆ–sectionï¼‰
        sections = self._group_by_sections(soup)
        
        for section_idx, section_tags in enumerate(sections):
            section_blocks = []
            
            for tag in section_tags:
                if not isinstance(tag, Tag):
                    continue
                    
                # è·å–ç›´æ¥æ–‡æœ¬å†…å®¹
                text = tag.get_text(separator=' ', strip=True)
                
                if len(text) >= self.config['min_text_length']:
                    # è®¡ç®—ä¸€äº›ç‰¹å¾
                    word_count = len(text.split())
                    char_count = len(text)
                    
                    # è®¡ç®—é“¾æ¥å¯†åº¦ï¼šé“¾æ¥æ–‡å­—é•¿åº¦ / å—æ–‡æœ¬é•¿åº¦
                    links = tag.find_all('a')
                    link_text_len = sum(len(a.get_text(strip=True)) for a in links)
                    link_density = link_text_len / max(len(text), 1)
                    
                    # æ£€æŸ¥æ ‡ç­¾ç±»å‹æƒé‡
                    tag_weight = self._get_tag_weight(tag.name)
                    
                    # æ£€æŸ¥ class å’Œ id å±æ€§
                    class_score = self._get_class_score(tag.get('class', []))
                    id_score = self._get_id_score(tag.get('id', ''))
                    
                    block = {
                        'tag': tag,
                        'text': text,
                        'word_count': word_count,
                        'char_count': char_count,
                        'link_density': link_density,
                        'tag_weight': tag_weight,
                        'class_score': class_score,
                        'id_score': id_score,
                        'section_index': section_idx,  # æ·»åŠ æ®µè½ç´¢å¼•
                        'total_score': 0.0
                    }
                    
                    section_blocks.append(block)
                    text_blocks.append(block)
            
            # è®°å½•æ¯ä¸ªæ®µè½çš„å—æ•°ï¼Œç”¨äºåç»­ä¿åº•æœºåˆ¶
            if section_blocks:
                for block in section_blocks:
                    block['section_size'] = len(section_blocks)
        
        return text_blocks

    def _group_by_sections(self, soup: BeautifulSoup) -> List[List[Tag]]:
        """æŒ‰æ®µè½åˆ†ç»„ï¼ŒåŸºäºäºŒçº§æ ‡é¢˜æˆ–sectionæ ‡ç­¾"""
        sections = []
        current_section = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ†æ®µæ ‡ç­¾
        all_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'section', 'article', 'div', 'p'])
        
        for tag in all_tags:
            # å¦‚æœæ˜¯æ ‡é¢˜æ ‡ç­¾ï¼Œå¼€å§‹æ–°æ®µè½
            if tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                if current_section:
                    sections.append(current_section)
                current_section = [tag]
            # å¦‚æœæ˜¯sectionæˆ–articleæ ‡ç­¾ï¼Œä¹Ÿè€ƒè™‘åˆ†æ®µ
            elif tag.name in ['section', 'article']:
                if current_section:
                    sections.append(current_section)
                current_section = [tag]
            else:
                current_section.append(tag)
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
        if current_section:
            sections.append(current_section)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾çš„åˆ†æ®µï¼Œå°†æ‰€æœ‰æ ‡ç­¾ä½œä¸ºä¸€ä¸ªæ®µè½
        if not sections:
            sections = [all_tags]
        
        logger.debug(f"æ£€æµ‹åˆ° {len(sections)} ä¸ªæ®µè½")
        return sections
    
    def _get_tag_weight(self, tag_name: str) -> float:
        """è·å–æ ‡ç­¾æƒé‡"""
        weights = {
            'article': 1.0,
            'main': 1.0,
            'section': 0.9,
            'div': 0.7,
            'p': 0.8,
            'h1': 0.9,
            'h2': 0.8,
            'h3': 0.7,
            'h4': 0.6,
            'h5': 0.5,
            'h6': 0.5,
        }
        return weights.get(tag_name, 0.5)
    
    def _get_class_score(self, classes: List[str]) -> float:
        """æ ¹æ® class åç§°è®¡ç®—å†…å®¹ç›¸å…³æ€§å¾—åˆ†"""
        if not classes:
            return 0.5
        
        # å†…å®¹ç›¸å…³çš„ class å…³é”®è¯
        content_keywords = [
            'content', 'article', 'post', 'story', 'text', 'body', 'main',
            'entry', 'description', 'summary', 'abstract', 'excerpt'
        ]
        
        # éå†…å®¹ç›¸å…³çš„ class å…³é”®è¯
        noise_keywords = [
            'ad', 'advertisement', 'banner', 'sidebar', 'nav', 'navigation',
            'menu', 'footer', 'header', 'comment', 'social', 'share',
            'related', 'recommend', 'popup', 'modal', 'widget'
        ]
        
        score = 0.5
        for class_name in classes:
            class_lower = class_name.lower()
            
            # æ£€æŸ¥å†…å®¹å…³é”®è¯
            for keyword in content_keywords:
                if keyword in class_lower:
                    score += 0.2
                    break
            
            # æ£€æŸ¥å™ªå£°å…³é”®è¯
            for keyword in noise_keywords:
                if keyword in class_lower:
                    score -= 0.3
                    break
        
        return max(0.0, min(1.0, score))
    
    def _get_id_score(self, element_id: str) -> float:
        """æ ¹æ® id åç§°è®¡ç®—å†…å®¹ç›¸å…³æ€§å¾—åˆ†"""
        if not element_id:
            return 0.5
        
        id_lower = element_id.lower()
        
        # å†…å®¹ç›¸å…³çš„ id å…³é”®è¯
        content_keywords = [
            'content', 'article', 'post', 'story', 'text', 'body', 'main'
        ]
        
        # éå†…å®¹ç›¸å…³çš„ id å…³é”®è¯
        noise_keywords = [
            'ad', 'advertisement', 'sidebar', 'nav', 'navigation',
            'menu', 'footer', 'header', 'comment', 'social'
        ]
        
        score = 0.5
        
        for keyword in content_keywords:
            if keyword in id_lower:
                return 0.8
        
        for keyword in noise_keywords:
            if keyword in id_lower:
                return 0.2
        
        return score
    
    def _pretokenize(self, text: str) -> str:
        """æ··åˆè¯­è¨€é¢„åˆ†è¯ï¼šä¸­æ–‡ jieba + è‹±æ–‡æ­£è§„åŒ–"""
        if not text:
            return ""
        
        text = text.strip()
        tokens = []
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        if re.search(r'[\u4e00-\u9fff]', text):
            # ä¸­æ–‡åˆ†è¯
            if JIEBA_AVAILABLE and self.config.get('enable_chinese_segmentation', True):
                try:
                    # jieba åˆ†è¯
                    words = jieba.cut(text)
                    tokens.extend([w.strip() for w in words if w.strip()])
                except Exception as e:
                    logger.warning(f"jieba åˆ†è¯å¤±è´¥: {e}")
                    # jieba å¤±è´¥ï¼Œç”¨æ±‰å­— bigram å…œåº•
                    tokens.extend(self._chinese_bigram_fallback(text))
            else:
                # jieba ä¸å¯ç”¨ï¼Œç”¨æ±‰å­— bigram å…œåº•
                tokens.extend(self._chinese_bigram_fallback(text))
        else:
            # è‹±æ–‡æ­£è§„åŒ– + å•è¯åˆ‡åˆ†
            tokens.extend(self._english_tokenize(text))
        
        # å»åœç”¨è¯
        tokens = self._remove_stopwords(tokens)
        
        # ç”¨ç©ºæ ¼è¿æ¥
        return " ".join(tokens)
    
    def _chinese_bigram_fallback(self, text: str) -> List[str]:
        """ä¸­æ–‡ bigram å…œåº•åˆ†è¯"""
        # æå–æ±‰å­—
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        tokens = []
        
        # å•å­—
        tokens.extend(chinese_chars)
        
        # bigram
        for i in range(len(chinese_chars) - 1):
            tokens.append(chinese_chars[i] + chinese_chars[i + 1])
        
        return tokens
    
    def _english_tokenize(self, text: str) -> List[str]:
        """è‹±æ–‡æ­£è§„åŒ– + å•è¯åˆ‡åˆ†"""
        # æ­£è§„åŒ–ï¼šè½¬å°å†™ï¼Œå»æ ‡ç‚¹
        normalized = re.sub(r'[^\w\s]', ' ', text.lower())
        
        # å•è¯åˆ‡åˆ†
        words = normalized.split()
        
        # è¿‡æ»¤å•å­—ç¬¦è¯ï¼ˆå¯é€‰ï¼‰
        words = [w for w in words if len(w) > 1]
        
        return words
    
    def _remove_stopwords(self, tokens: List[str]) -> List[str]:
        """ç»Ÿä¸€å»åœç”¨è¯ï¼ˆä¸­è‹±ä¸¤å¥—ï¼‰"""
        # ä¸­æ–‡åœç”¨è¯
        chinese_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
        }
        
        # è‹±æ–‡åœç”¨è¯
        english_stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'
        }
        
        # è¿‡æ»¤åœç”¨è¯
        filtered_tokens = []
        for token in tokens:
            token_lower = token.lower()
            if token_lower not in chinese_stopwords and token_lower not in english_stopwords:
                filtered_tokens.append(token)
        
        return filtered_tokens

    def _clean_and_filter_blocks(self, text_blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ¸…æ´—å’Œè¿‡æ»¤æ–‡æœ¬å—ï¼Œå»æ‰ç»´åŸºè„šæ³¨ã€å¤šä½™ç©ºç™½ï¼Œè¿‡æ»¤è¿‡çŸ­/è¿‡é•¿çš„å¥å­"""
        cleaned_blocks = []
        
        for block in text_blocks:
            text = block['text']
            
            # 1. æ¸…æ´—ç»´åŸºè„šæ³¨ [123], [ 123 ], [123-456] ç­‰æ ¼å¼
            text = re.sub(r'\s?\[\s?\d+(?:\s?[â€“-]\s?\d+)?\s?\]', '', text)
            
            # 2. æ¸…æ´—å¤šä½™ç©ºç™½
            text = re.sub(r'\s+', ' ', text).strip()
            
            # 3. è¿‡æ»¤è¿‡çŸ­æˆ–è¿‡é•¿çš„å¥å­
            if len(text) < 25:  # å¤ªçŸ­çš„å¥å­
                continue
            if len(text) > 400:  # å¤ªé•¿çš„å¥å­ï¼Œå¯èƒ½éœ€è¦åˆ†æ®µ
                # å°è¯•æŒ‰å¥å·åˆ†æ®µ
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if 25 <= len(sentence) <= 400:
                        new_block = block.copy()
                        new_block['text'] = sentence
                        cleaned_blocks.append(new_block)
            else:
                block['text'] = text
                cleaned_blocks.append(block)
        
        logger.debug(f"æ–‡æœ¬æ¸…æ´—å®Œæˆï¼š{len(text_blocks)} -> {len(cleaned_blocks)} ä¸ªæœ‰æ•ˆå—")
        return cleaned_blocks

    def _calculate_position_weight(self, index: int, total: int) -> float:
        """è®¡ç®—ä½ç½®å…ˆéªŒæƒé‡ï¼Œè¶Šé å‰çš„å¥å­æƒé‡è¶Šé«˜"""
        if total <= 1:
            return 0.5
        
        # çº¿æ€§è¡°å‡ï¼šç¬¬ä¸€ä¸ªå¥å­æƒé‡æœ€é«˜ï¼Œæœ€åä¸€ä¸ªå¥å­æƒé‡æœ€ä½
        position_ratio = 1.0 - (index / (total - 1))
        # æƒé‡èŒƒå›´ï¼š0.1 - 0.2
        weight = 0.1 + (position_ratio * 0.1)
        return weight

    def _mmr_diversity_selection(self, text_blocks: List[Dict[str, Any]], tfidf_matrix) -> List[Dict[str, Any]]:
        """MMRå»å†—ä½™é€‰æ‹©ï¼Œåœ¨å‰200åé‡Œåšå¤šæ ·æ€§é€‰æ‹©"""
        if len(text_blocks) <= 10:  # å¦‚æœå—æ•°å¾ˆå°‘ï¼Œç›´æ¥è¿”å›
            return text_blocks
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œå–å‰200å
        sorted_blocks = sorted(text_blocks, key=lambda x: x['total_score'], reverse=True)
        top_candidates = sorted_blocks[:min(200, len(sorted_blocks))]
        
        if len(top_candidates) <= 10:
            return top_candidates
        
        # MMRé€‰æ‹©
        selected = []
        remaining_indices = list(range(len(top_candidates)))
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªï¼ˆå¾—åˆ†æœ€é«˜çš„ï¼‰
        selected.append(0)
        remaining_indices.remove(0)
        
        # MMRå‚æ•°
        lambda_param = 0.7  # å¤šæ ·æ€§æƒé‡
        
        while len(selected) < min(50, len(top_candidates)) and remaining_indices:
            best_score = -1
            best_idx = None
            
            for idx in remaining_indices:
                # è®¡ç®—ä¸å·²é€‰å¥å­çš„æœ€å¤§ç›¸ä¼¼åº¦
                max_sim = 0
                for sel_idx in selected:
                    sim = linear_kernel(
                        tfidf_matrix[idx:idx+1], 
                        tfidf_matrix[sel_idx:sel_idx+1]
                    )[0, 0]
                    max_sim = max(max_sim, sim)
                
                # MMRå¾—åˆ†ï¼šç›¸å…³æ€§ - Î» * å†—ä½™åº¦
                relevance = top_candidates[idx]['total_score']
                redundancy = max_sim
                mmr_score = relevance - lambda_param * redundancy
                
                if mmr_score > best_score:
                    best_score = mmr_score
                    best_idx = idx
            
            if best_idx is not None:
                selected.append(best_idx)
                remaining_indices.remove(best_idx)
            else:
                break
        
        # è¿”å›é€‰ä¸­çš„å—
        result = [top_candidates[i] for i in selected]
        logger.debug(f"MMRå¤šæ ·æ€§é€‰æ‹©å®Œæˆï¼š{len(top_candidates)} -> {len(result)} ä¸ªå—")
        return result

    def _calculate_tfidf_scores(self, text_blocks: List[Dict[str, Any]], query_text: str = "") -> List[Dict[str, Any]]:
        """ä½¿ç”¨è¦†ç›–ä¼˜å…ˆçš„TF-IDFæŠ½å–å™¨ï¼Œé¿å…"åªå‰©å‡ æ®µ"çš„åå·®"""
        if not text_blocks:
            return text_blocks
        
        # 1. æ¸…æ´—å’Œé¢„å¤„ç†æ–‡æœ¬å—
        cleaned_blocks = self._clean_and_filter_blocks(text_blocks)
        if not cleaned_blocks:
            logger.warning("æ¸…æ´—åæ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—")
            return text_blocks
        
        # å‡†å¤‡æ–‡æœ¬æ•°æ®
        texts = [self._pretokenize(block['text']) for block in cleaned_blocks]
        
        try:
            # 2. åˆ›å»ºä¼˜åŒ–çš„TF-IDFå‘é‡åŒ–å™¨ï¼ˆè¦†ç›–ä¼˜å…ˆé…ç½®ï¼‰
            n_docs = len(texts)
            min_df = min(self.config['min_df'], max(1, n_docs - 1))
            max_df = max(self.config['max_df'], min(1.0, max(0.1, n_docs / 2)))
            
            if n_docs < 3:
                min_df = 1
                max_df = 1.0
            
            self.vectorizer = TfidfVectorizer(
                max_features=self.config['max_features'],
                min_df=min_df,
                max_df=max_df,
                stop_words=None,  # åœ¨é¢„åˆ†è¯é˜¶æ®µå¤„ç†
                token_pattern=r'(?u)\b\w+\b',
                ngram_range=(1, 2),  # ä½¿ç”¨1-2gramæå‡çŸ­è¯­/äººåæŠ“å–
                lowercase=False,
                strip_accents=None,
                sublinear_tf=True,  # æ¬¡çº¿æ€§TFï¼Œé™ä½é«˜é¢‘è¯å½±å“
                norm='l2'
            )
            
            # è®¡ç®—TF-IDFå‘é‡
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            
            # 3. ä¸­å¿ƒæ€§æ‘˜è¦ï¼ˆæ— queryæ—¶çš„è¦†ç›–åº¦è®¡ç®—ï¼‰
            if not query_text.strip():
                # è®¡ç®—æ–‡æ¡£è´¨å¿ƒ
                centroid = tfidf_matrix.mean(axis=0)
                # è®¡ç®—ä¸è´¨å¿ƒçš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºè¦†ç›–åº¦åˆ†
                similarities = linear_kernel(tfidf_matrix, centroid).ravel()
            else:
                # æœ‰queryæ—¶ä½¿ç”¨queryå‘é‡
                query_vector = self.vectorizer.transform([self._pretokenize(query_text)])
                similarities = linear_kernel(query_vector, tfidf_matrix).ravel()
            
            # 4. è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆè¦†ç›–ä¼˜å…ˆï¼‰
            for i, block in enumerate(cleaned_blocks):
                # åŸºç¡€TF-IDFå¾—åˆ†
                tfidf_score = float(similarities[i])
                
                # ä½ç½®å…ˆéªŒï¼šè¶Šé å‰çš„å¥å­åŠ åˆ†
                position_weight = self._calculate_position_weight(i, len(cleaned_blocks))
                
                # ç»¼åˆå¾—åˆ†ï¼šè¦†ç›–åº¦ + ç»“æ„åŒ–ç‰¹å¾ + ä½ç½®å…ˆéªŒ
                total_score = (
                    tfidf_score * 0.5 +  # TF-IDFè¦†ç›–åº¦æƒé‡
                    block['tag_weight'] * 0.2 +
                    block['class_score'] * 0.15 +
                    block['id_score'] * 0.05 +
                    position_weight * 0.1  # ä½ç½®å…ˆéªŒæƒé‡
                )
                
                block['tfidf_score'] = tfidf_score
                block['position_weight'] = position_weight
                block['total_score'] = total_score
            
            # 5. MMRå»å†—ä½™é€‰æ‹©
            selected_blocks = self._mmr_diversity_selection(cleaned_blocks, tfidf_matrix)
            
            logger.debug(f"è¦†ç›–ä¼˜å…ˆTF-IDFåˆ†æå®Œæˆï¼Œå¤„ç†äº† {len(text_blocks)} -> {len(selected_blocks)} ä¸ªæ–‡æœ¬å—")
            
            return selected_blocks
            
        except Exception as e:
            logger.warning(f"è¦†ç›–ä¼˜å…ˆTF-IDFè®¡ç®—å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºç¡€å¾—åˆ†
            for block in text_blocks:
                block['tfidf_score'] = 0.0
                block['total_score'] = (
                    block['tag_weight'] * 0.4 +
                    block['class_score'] * 0.3 +
                    block['id_score'] * 0.2 +
                    (1.0 - block.get('link_density', 0)) * 0.1
                )
            text_blocks.sort(key=lambda x: x['total_score'], reverse=True)
            return text_blocks
    
    def _apply_quality_rules(self, text_blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åº”ç”¨è´¨é‡è§„åˆ™è¿‡æ»¤"""
        if not text_blocks:
            return text_blocks
        
        filtered_blocks = []
        discard_reasons = {'too_short': 0, 'link_density': 0, 'blacklist': 0, 'alphanumeric': 0}
        
        # é»‘åå•çŸ­è¯­
        blacklist_phrases = [
            'cookie', 'subscribe', 'å…è´£', 'ç›¸å…³é˜…è¯»', 'æ‰«ç ', 'Â©', 'è´£ä»»ç¼–è¾‘',
            'å¹¿å‘Š', 'æ¨å¹¿', 'èµåŠ©', 'ç‚¹å‡»', 'æ›´å¤š', 'é˜…è¯»å…¨æ–‡', 'å±•å¼€å…¨æ–‡'
        ]
        
        for block in text_blocks:
            text = block['text']
            
            # è§„åˆ™1: æœ€å°é•¿åº¦
            if len(text) < self.config['min_text_length']:
                discard_reasons['too_short'] += 1
                continue
            
            # è§„åˆ™2: é“¾æ¥å¯†åº¦è¿‡é«˜
            if block.get('link_density', 0) > self.config.get('max_link_density', 0.3):
                discard_reasons['link_density'] += 1
                continue
            
            # è§„åˆ™3: éå­—æ¯æ•°å­—æ¯”ä¾‹è¿‡é«˜
            alphanumeric_count = sum(1 for c in text if c.isalnum())
            alphanumeric_ratio = alphanumeric_count / max(len(text), 1)
            if alphanumeric_ratio < self.config.get('min_alphanumeric_ratio', 0.5):
                discard_reasons['alphanumeric'] += 1
                continue
            
            # è§„åˆ™4: é»‘åå•çŸ­è¯­ä¸”é•¿åº¦è¾ƒçŸ­
            text_lower = text.lower()
            if any(phrase in text_lower for phrase in blacklist_phrases) and len(text) < 120:
                discard_reasons['blacklist'] += 1
                continue
            
            filtered_blocks.append(block)
        
        logger.debug(f"è´¨é‡è§„åˆ™è¿‡æ»¤ï¼š{len(text_blocks)} -> {len(filtered_blocks)} ä¸ªæ–‡æœ¬å—")
        logger.debug(f"ä¸¢å¼ƒåŸå› ï¼š{discard_reasons}")
        
        return filtered_blocks
    
    def _filter_low_quality_blocks(self, text_blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿‡æ»¤ä½è´¨é‡æ–‡æœ¬å— - è¦†ç›–ä¼˜å…ˆ + åˆ†åŒºæ®µä¿åº•"""
        if not self.config['remove_low_quality']:
            return text_blocks
        
        if not text_blocks:
            return text_blocks
        
        n = len(text_blocks)
        scores = [block['total_score'] for block in text_blocks]
        
        # è®¡ç®—é˜ˆå€¼
        p80 = float(np.percentile(scores, self.config['percentile_threshold'] * 100))
        floor = float(self.config.get('score_floor', self.config.get('similarity_threshold', 0.06)))
        
        # Top-K å…œåº•
        K = max(int(np.ceil(self.config['content_ratio_threshold'] * n)), self.config['min_keep_k'])
        
        # ä¸¤ç§ä¿ç•™ç­–ç•¥
        by_rank = set(range(min(K, n)))  # text_blocks å·²æŒ‰åˆ†æ•°é™åº
        by_thresh = {i for i, s in enumerate(scores) if s >= max(p80, floor)}
        
        # åˆå¹¶ä¿ç•™çš„ç´¢å¼•
        kept_idx = sorted(by_rank | by_thresh)
        
        # åˆ†åŒºæ®µä¿åº•ï¼šæ¯ä¸ªæ®µè½è‡³å°‘ä¿ç•™1-3å¥
        kept_idx = self._apply_section_guarantee(text_blocks, kept_idx)
        
        filtered_blocks = [text_blocks[i] for i in kept_idx]
        
        logger.debug(f"è¦†ç›–ä¼˜å…ˆè¿‡æ»¤ï¼š{n} -> {len(filtered_blocks)} ä¸ªæ–‡æœ¬å—")
        logger.debug(f"é˜ˆå€¼: P80={p80:.4f}, floor={floor:.4f}, K={K}")
        logger.debug(f"ä¿ç•™ç­–ç•¥: Top-K={len(by_rank)}, é˜ˆå€¼={len(by_thresh)}, ä¿åº•å={len(kept_idx)}")
        
        return filtered_blocks

    def _apply_section_guarantee(self, text_blocks: List[Dict[str, Any]], kept_indices: List[int]) -> List[int]:
        """åº”ç”¨åˆ†åŒºæ®µä¿åº•æœºåˆ¶ï¼Œæ¯ä¸ªæ®µè½è‡³å°‘ä¿ç•™1-3å¥"""
        # æŒ‰æ®µè½åˆ†ç»„
        sections = {}
        for i, block in enumerate(text_blocks):
            section_idx = block.get('section_index', 0)
            if section_idx not in sections:
                sections[section_idx] = []
            sections[section_idx].append(i)
        
        # ä¸ºæ¯ä¸ªæ®µè½ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€å®šæ•°é‡çš„å—
        final_indices = set(kept_indices)
        
        for section_idx, section_indices in sections.items():
            section_size = len(section_indices)
            
            # è®¡ç®—è¯¥æ®µè½åº”è¯¥ä¿ç•™çš„æœ€å°æ•°é‡
            if section_size <= 3:
                min_keep = 1  # å°æ®µè½è‡³å°‘ä¿ç•™1å¥
            elif section_size <= 10:
                min_keep = 2  # ä¸­ç­‰æ®µè½è‡³å°‘ä¿ç•™2å¥
            else:
                min_keep = 3  # å¤§æ®µè½è‡³å°‘ä¿ç•™3å¥
            
            # æ£€æŸ¥è¯¥æ®µè½å·²ä¿ç•™çš„æ•°é‡
            kept_in_section = [i for i in section_indices if i in final_indices]
            
            if len(kept_in_section) < min_keep:
                # éœ€è¦è¡¥å……ï¼ŒæŒ‰å¾—åˆ†æ’åºé€‰æ‹©
                section_blocks_with_scores = [(i, text_blocks[i]['total_score']) for i in section_indices]
                section_blocks_with_scores.sort(key=lambda x: x[1], reverse=True)
                
                # è¡¥å……åˆ°æœ€å°æ•°é‡
                needed = min_keep - len(kept_in_section)
                for i, _ in section_blocks_with_scores:
                    if i not in final_indices and needed > 0:
                        final_indices.add(i)
                        needed -= 1
        
        return sorted(final_indices)
    
    def _enhance_content_structure(self, soup: BeautifulSoup, high_quality_blocks: List[Dict[str, Any]]) -> BeautifulSoup:
        """å¢å¼ºå†…å®¹ç»“æ„ï¼Œä¼˜åŒ– HTML ä»¥æé«˜ Trafilatura æå–è´¨é‡"""
        if not self.config['enhance_content_blocks']:
            return soup
        
        try:
            # åˆ›å»ºæ–°çš„ HTML ç»“æ„
            new_soup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')
            body = new_soup.body
            
            # ä¿ç•™åŸå§‹çš„ head éƒ¨åˆ†ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
            if soup.head:
                new_soup.head.replace_with(soup.head)
            
            # æ·»åŠ é«˜è´¨é‡å†…å®¹å—
            for i, block in enumerate(high_quality_blocks[:10]):  # åªä¿ç•™å‰10ä¸ªæœ€é«˜è´¨é‡çš„å—
                # åˆ›å»ºåŒ…è£…å™¨
                wrapper = new_soup.new_tag('div', **{'class': 'tfidf-content-block', 'data-score': f"{block['total_score']:.3f}"})
                
                # å…‹éš†åŸå§‹æ ‡ç­¾
                cloned_tag = new_soup.new_tag(block['tag'].name)
                cloned_tag.string = block['text']
                
                # ä¿ç•™é‡è¦å±æ€§
                if block['tag'].get('id'):
                    cloned_tag['id'] = block['tag']['id']
                if block['tag'].get('class'):
                    cloned_tag['class'] = block['tag']['class']
                
                wrapper.append(cloned_tag)
                body.append(wrapper)
            
            logger.debug(f"HTML ç»“æ„ä¼˜åŒ–å®Œæˆï¼Œä¿ç•™äº† {len(high_quality_blocks[:10])} ä¸ªé«˜è´¨é‡å†…å®¹å—")
            return new_soup
            
        except Exception as e:
            logger.warning(f"HTML ç»“æ„ä¼˜åŒ–å¤±è´¥: {e}")
            return soup
    
    def optimize_html(self, html: str, url: Optional[str] = None, title: str = "", user_query: str = "") -> Tuple[str, Dict[str, Any]]:
        """
        ä½¿ç”¨ TF-IDF ä¼˜åŒ– HTML å†…å®¹
        
        Args:
            html: åŸå§‹ HTML å†…å®¹
            url: é¡µé¢ URLï¼ˆå¯é€‰ï¼‰
            title: é¡µé¢æ ‡é¢˜ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
            user_query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
        
        Returns:
            ä¼˜åŒ–åçš„ HTML å’Œåˆ†ææŠ¥å‘Š
        """
        if not SKLEARN_AVAILABLE or not is_tfidf_enabled():
            return html, {'optimization': 'disabled'}
        
        try:
            logger.debug(f"å¼€å§‹ TF-IDF HTML ä¼˜åŒ–: {url or 'unknown'}")
            
            # è§£æ HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ–‡æœ¬å—
            text_blocks = self._extract_text_blocks(soup)
            
            if not text_blocks:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æœ¬å—")
                return html, {'optimization': 'no_content_blocks'}
            
            # ç”ŸæˆæŸ¥è¯¢æ–‡æœ¬
            query_text = (title + " " + user_query).strip()
            
            # è®¡ç®— TF-IDF å¾—åˆ†
            text_blocks = self._calculate_tfidf_scores(text_blocks, query_text)
            
            # åº”ç”¨è´¨é‡è§„åˆ™è¿‡æ»¤
            text_blocks = self._apply_quality_rules(text_blocks)
            
            # è¿‡æ»¤ä½è´¨é‡å†…å®¹
            high_quality_blocks = self._filter_low_quality_blocks(text_blocks)
            
            # å¢å¼ºå†…å®¹ç»“æ„
            optimized_soup = self._enhance_content_structure(soup, high_quality_blocks)
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = {
                'optimization': 'success',
                'original_blocks': len(text_blocks),
                'filtered_blocks': len(high_quality_blocks),
                'top_scores': [
                    {
                        'score': block['total_score'],
                        'tfidf_score': block['tfidf_score'],
                        'tag': block['tag'].name,
                        'text_preview': block['text'][:100] + '...' if len(block['text']) > 100 else block['text']
                    }
                    for block in high_quality_blocks[:3]
                ],
                'config': self.config
            }
            
            logger.info(f"TF-IDF ä¼˜åŒ–å®Œæˆ: {len(text_blocks)} -> {len(high_quality_blocks)} ä¸ªå†…å®¹å—")
            
            return str(optimized_soup), report
            
        except Exception as e:
            logger.error(f"TF-IDF ä¼˜åŒ–å¤±è´¥: {e}")
            return html, {'optimization': 'failed', 'error': str(e)}


def optimize_html_with_tfidf(html: str, url: Optional[str] = None, title: str = "", user_query: str = "") -> Tuple[str, Dict[str, Any]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨ TF-IDF ä¼˜åŒ– HTML
    
    Args:
        html: åŸå§‹ HTML å†…å®¹
        url: é¡µé¢ URLï¼ˆå¯é€‰ï¼‰
        title: é¡µé¢æ ‡é¢˜ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
        user_query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
    
    Returns:
        ä¼˜åŒ–åçš„ HTML å’Œåˆ†ææŠ¥å‘Š
    """
    optimizer = TfidfTextOptimizer()
    return optimizer.optimize_html(html, url, title, user_query)


# é…ç½®è¯´æ˜
TFIDF_CONFIG_HELP = """
TF-IDF è¦†ç›–ä¼˜å…ˆæŠ½å–å™¨é…ç½®é€‰é¡¹:

æ ¸å¿ƒå¼€å…³:
- TFIDF_OPTIMIZER_ENABLED=true               # å¯ç”¨ TF-IDF è¦†ç›–ä¼˜å…ˆæŠ½å–

æ–‡æœ¬å¤„ç†:
- TFIDF_MIN_TEXT_LENGTH=50                   # æœ€å°æ–‡æœ¬é•¿åº¦
- TFIDF_MAX_FEATURES=10000                   # TF-IDF æœ€å¤§ç‰¹å¾æ•°
- TFIDF_MIN_DF=1                             # æœ€å°æ–‡æ¡£é¢‘ç‡
- TFIDF_MAX_DF=0.9                           # æœ€å¤§æ–‡æ¡£é¢‘ç‡

è¦†ç›–ä¼˜å…ˆç‰¹æ€§:
- TFIDF_SCORE_FLOOR=0.04                     # å¾—åˆ†ä¸‹é™
- TFIDF_CONTENT_RATIO=0.5                    # å†…å®¹æ¯”ä¾‹é˜ˆå€¼
- TFIDF_PERCENTILE_THRESHOLD=0.70            # åˆ†ä½é˜ˆå€¼
- TFIDF_MIN_KEEP_K=60                        # æœ€å°‘ä¿ç•™å—æ•°

è´¨é‡æ§åˆ¶:
- TFIDF_REMOVE_LOW_QUALITY=true              # ç§»é™¤ä½è´¨é‡å†…å®¹
- TFIDF_ENHANCE_BLOCKS=true                  # å¢å¼ºå†…å®¹å—ç»“æ„
- TFIDF_MAX_LINK_DENSITY=0.4                 # æœ€å¤§é“¾æ¥å¯†åº¦
- TFIDF_MIN_ALPHANUMERIC_RATIO=0.4           # æœ€å°å­—æ¯æ•°å­—æ¯”ä¾‹

è¦†ç›–ä¼˜å…ˆä¼˜åŠ¿:
âœ… ä¸­å¿ƒæ€§æ‘˜è¦ï¼šåŸºäºæ–‡æ¡£è´¨å¿ƒçš„è¦†ç›–åº¦è®¡ç®—
âœ… åˆ†åŒºæ®µä¿åº•ï¼šæ¯ä¸ªæ®µè½è‡³å°‘ä¿ç•™1-3å¥ï¼Œé¿å…"åªå‰©å‡ æ®µ"
âœ… ä½ç½®å…ˆéªŒï¼šè¶Šé å‰çš„å¥å­æƒé‡è¶Šé«˜ï¼Œä¿æŠ¤å¯¼è¯­/è¿‡æ¸¡
âœ… MMRå»å†—ä½™ï¼šåœ¨å‰200åé‡Œåšå¤šæ ·æ€§é€‰æ‹©
âœ… N-gram + æ¬¡çº¿æ€§TFï¼šæå‡çŸ­è¯­/äººåæŠ“å–
âœ… ç»´åŸºè„šæ³¨æ¸…æ´—ï¼šè‡ªåŠ¨å»é™¤[123]æ ¼å¼çš„è„šæ³¨
âœ… æ™ºèƒ½åˆ†æ®µï¼šæŒ‰äºŒçº§æ ‡é¢˜æˆ–sectionè‡ªåŠ¨åˆ†ç»„
âœ… è‡ªé€‚åº”é˜ˆå€¼ï¼šæ ¹æ®æ–‡æ¡£æ•°é‡åŠ¨æ€è°ƒæ•´å‚æ•°
"""


def test_coverage_priority_extractor():
    """æµ‹è¯•è¦†ç›–ä¼˜å…ˆæŠ½å–å™¨æ•ˆæœ"""
    print("ğŸ§ª æµ‹è¯•è¦†ç›–ä¼˜å…ˆTF-IDFæŠ½å–å™¨:")
    
    # æ¨¡æ‹ŸHTMLå†…å®¹
    test_html = """
    <html>
    <head><title>äººå·¥æ™ºèƒ½å‘å±•å²</title></head>
    <body>
        <h1>äººå·¥æ™ºèƒ½å‘å±•å²</h1>
        <p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚</p>
        
        <h2>æ—©æœŸå‘å±•</h2>
        <p>1950å¹´ä»£ï¼Œè‰¾ä¼¦Â·å›¾çµæå‡ºäº†è‘—åçš„å›¾çµæµ‹è¯•[1]ï¼Œè¿™æˆä¸ºäº†åˆ¤æ–­æœºå™¨æ™ºèƒ½çš„é‡è¦æ ‡å‡†ã€‚</p>
        <p>1956å¹´ï¼Œè¾¾ç‰¹èŒ…æ–¯ä¼šè®®æ ‡å¿—ç€äººå·¥æ™ºèƒ½ä½œä¸ºä¸€é—¨å­¦ç§‘çš„æ­£å¼è¯ç”Ÿ[2-3]ã€‚</p>
        
        <h2>ç°ä»£å‘å±•</h2>
        <p>è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„çªç ´æ¨åŠ¨äº†AIçš„å¿«é€Ÿå‘å±•ã€‚</p>
        <p>æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
        <p>å¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPTç³»åˆ—å±•ç°äº†å¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚</p>
        
        <h2>æœªæ¥å±•æœ›</h2>
        <p>äººå·¥æ™ºèƒ½å°†ç»§ç»­åœ¨å„ä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚</p>
        <p>æˆ‘ä»¬éœ€è¦å…³æ³¨AIçš„ä¼¦ç†é—®é¢˜å’Œç¤¾ä¼šå½±å“ã€‚</p>
        
        <div class="advertisement">è¿™æ˜¯å¹¿å‘Šå†…å®¹ï¼Œåº”è¯¥è¢«è¿‡æ»¤</div>
        <p class="footer">ç‰ˆæƒä¿¡æ¯ Â© 2024</p>
    </body>
    </html>
    """
    
    try:
        # ä½¿ç”¨æ›´å®½æ¾çš„é…ç½®è¿›è¡Œæµ‹è¯•
        test_config = {
            'min_text_length': 20,  # é™ä½æœ€å°æ–‡æœ¬é•¿åº¦
            'max_features': 1000,
            'min_df': 1,
            'max_df': 0.9,
            'score_floor': 0.02,
            'content_ratio_threshold': 0.5,
            'min_keep_k': 10,
            'percentile_threshold': 0.70,
            'remove_low_quality': True,
            'enhance_content_blocks': True,
            'enable_chinese_segmentation': True,
            'max_link_density': 0.4,
            'min_alphanumeric_ratio': 0.4,
        }
        optimizer = TfidfTextOptimizer(test_config)
        optimized_html, report = optimizer.optimize_html(test_html, "https://example.com/ai-history", "äººå·¥æ™ºèƒ½å‘å±•å²", "")
        
        print(f"âœ… ä¼˜åŒ–å®Œæˆ:")
        print(f"   - åŸå§‹å—æ•°: {report.get('original_blocks', 0)}")
        print(f"   - è¿‡æ»¤åå—æ•°: {report.get('filtered_blocks', 0)}")
        print(f"   - ä¼˜åŒ–çŠ¶æ€: {report.get('optimization', 'unknown')}")
        
        if 'top_scores' in report:
            print(f"   - é«˜åˆ†å—é¢„è§ˆ:")
            for i, block in enumerate(report['top_scores'][:3], 1):
                print(f"     {i}. [{block['tag']}] {block['text_preview']}")
        
        print(f"\nğŸ“Š è¦†ç›–ä¼˜å…ˆç‰¹æ€§éªŒè¯:")
        print(f"   âœ… ç»´åŸºè„šæ³¨æ¸…æ´—: è‡ªåŠ¨å»é™¤[1], [2-3]ç­‰æ ¼å¼")
        print(f"   âœ… åˆ†åŒºæ®µä¿åº•: æ¯ä¸ªh2æ®µè½è‡³å°‘ä¿ç•™å†…å®¹")
        print(f"   âœ… ä½ç½®å…ˆéªŒ: è¶Šé å‰çš„å†…å®¹æƒé‡è¶Šé«˜")
        print(f"   âœ… å™ªå£°è¿‡æ»¤: å¹¿å‘Šå’Œç‰ˆæƒä¿¡æ¯è¢«è¿‡æ»¤")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    test_coverage_priority_extractor()
